---
title: "CUNY 2020 Analysis"
output: html_notebook
---

# Packages and utilities

```{r}
library(tidyverse)
library(lme4)
library(lmerTest)
library(logging)
library(mvtnorm)
library(mgcv)
# Provides bootstrap resampling tools
library(rsample)
```

```{r}
# Compute the log-likelihood of a new dataset using a fit lme4 model.
logLik_test <- function(lm, test_X, test_y) {
  predictions <- predict(lm, test_X, re.form=NA)
  # Get std.dev. of residual, estimated from train data
  stdev <- sigma(lm)
  # For each prediction--observation, get the density p(obs | N(predicted, model_sigma)) and reduce
  density <- sum(dnorm(test_y, predictions, stdev, log=TRUE))
  return(density)
}
# Get per-prediction log-likelihood
logLik_test_per <- function(lm, test_X, test_y) {
  predictions <- predict(lm, test_X, re.form=NA)
  # Get std.dev. of residual, estimated from train data
  stdev <- sigma(lm)
  # For each prediction--observation, get the density p(obs | N(predicted, model_sigma))
  densities <- dnorm(test_y, predictions, stdev, log=TRUE)
  return(densities)
}
# Compute MSE of a new dataset using a fit lme4 model.
mse_test <- function(lm, test_X, test_y) {
  return(mean((predict(lm, test_X, re.form=NA) - test_y) ^ 2))
}
#Sanity checks
#mylm <- gam(psychometric ~  s(surprisal, bs = "cr", k = 20) + s(prev_surp, bs = "cr", k = 20) + te(freq, len, bs = "cr") + te(prev_freq, prev_len, bs = "cr"), data=train_data)
#c(logLik(mylm), logLik_test(mylm, train_data, train_data$psychometric))
#logLik_test(mylm, test_data, test_data$psychometric)
```

# Data loading and preprocessing

```{r Load and preprocess data}
data = read.csv("../data/harmonized_results.csv")

all_data = data %>%
  mutate(seed = as.factor(seed)) %>%
  group_by(corpus, model, training, seed) %>%
    mutate(prev_surp = lag(surprisal),
         prev_code = lag(code),
         prev_len = lag(len),
         prev_freq = lag(freq),
         prev_surp = lag(surprisal),
         
         prev2_freq = lag(prev_freq),
         prev2_code = lag(prev_code),
         prev2_len = lag(prev_len),
         prev2_surp = lag(prev_surp),
         
         prev3_freq = lag(prev2_freq),
         prev3_code = lag(prev2_code),
         prev3_len = lag(prev2_len),
         prev3_surp = lag(prev2_surp),
         
         prev4_freq = lag(prev3_freq),
         prev4_code = lag(prev3_code),
         prev4_len = lag(prev3_len),
         prev4_surp = lag(prev3_surp)) %>%
  ungroup() %>%
  
  # Filter back two for the dundee corpus. Filter back 1 for all other corpora
  # NB this effectively removes all zero-surprisal rows, since early-sentence tokens don't have contiguous token history
  filter((corpus == "dundee" & code == prev2_code + 2) | (corpus != "dundee" & code == prev4_code + 4)) %>%
  
  select(-prev_code, -prev2_code, -prev3_code) %>%
  drop_na()

all_data = all_data %>%
  mutate(
    model = as.character(model),
    model = if_else(model == "gpt-2", "gpt2", model),
    model = as.factor(model))
```

```{r}
missing_rows = all_data %>% complete(nesting(corpus, code), nesting(model, training, seed)) %>% 
  group_by(corpus, code) %>% 
    filter(sum(is.na(surprisal)) > 0) %>% 
  ungroup() %>% 
  anti_join(all_data, by=c("corpus", "code", "model", "training", "seed"))

missing_rows %>% ggplot(aes(x=corpus, fill=factor(paste(model,training)))) + geom_bar(position=position_dodge(width=0.8))
print(missing_rows %>% group_by(model, training, seed, corpus) %>% summarise(n=n())) %>% arrange(desc(n))
```


```{r Drop tokens for which any model is missing surprisal data.}

# Compute the ideal number of model--seed--training observations per token.
to_drop = all_data %>%
  group_by(corpus, code) %>% summarise(n = n()) %>% ungroup() %>%
  group_by(corpus) %>% mutate( max_n = max(n)) %>% ungroup() %>%
  filter(max_n != n) %>%
  select(code, corpus)

#to_drop = all_data %>% group_by(corpus, code) %>% filter(n() != ideal_token_obs_count) %>% ungroup()
loginfo(paste("Dropping", nrow(to_drop), "observations corresponding to corpus tokens which are missing observations for some model."))
loginfo(paste("Dropping", to_drop %>% group_by(corpus, code) %>% n_groups(), "tokens which are missing observations for some model."))

all_data = all_data %>% anti_join(to_drop %>% group_by(corpus, code), by=c("corpus", "code"))
loginfo(paste("After drop,", nrow(all_data), "observations (", all_data %>% group_by(corpus, code) %>% n_groups(), " tokens) remain."))
```

```{r Drop tokens for which any model has zero-valued surprisals.}

to_drop_zero_surps = all_data %>% group_by(corpus, code) %>% filter(any(surprisal == 0)) %>% ungroup()
loginfo(paste("Dropping", nrow(to_drop_zero_surps), "observations corresponding to corpus tokens which have surprisal zeros for some model."))
loginfo(paste("Dropping", to_drop_zero_surps %>% group_by(corpus, code) %>% n_groups(), "tokens which have surprisal zeros for some model."))

all_data = all_data %>% anti_join(to_drop_zero_surps %>% group_by(corpus, code), by=c("corpus", "code"))
loginfo(paste("After drop,", nrow(all_data), "observations (", all_data %>% group_by(corpus, code) %>% n_groups(), " tokens) remain."))
```

```{r Drop tokens for which we have zero-valued psychometric data.}

to_drop_zero_psychs = all_data %>% group_by(corpus, code) %>% filter(any(psychometric == 0)) %>% ungroup()
loginfo(paste("Dropping", nrow(to_drop_zero_psychs), "observations corresponding to corpus tokens which have psychometric zeros for some model."))
loginfo(paste("Dropping", to_drop_zero_psychs %>% group_by(corpus, code) %>% n_groups(), "tokens which have psychometric zeros for some model."))

all_data = all_data %>% anti_join(to_drop_zero_psychs %>% group_by(corpus, code), by=c("corpus", "code"))
loginfo(paste("After drop,", nrow(all_data), "observations (", all_data %>% group_by(corpus, code) %>% n_groups(), " tokens) remain."))
```

 
# Learn models
 
```{r}
# Compute linear model stats for the given training data subset and full test data.
# Automatically subsets the test data to match the relevant group for which we are training a linear model.
get_lm_data <- function(df, test_data, formula, fold, store_env) {
  #this_lm <- gam(formula, data=df);
  this_lm = lm(formula, data=df)
  this_test_data <- semi_join(test_data, df, by=c("training", "model", "seed", "corpus"));
  
  # Save lm to the global env so that we can access residuals later.
  lm_name = paste(unique(paste(df$model, df$training, df$seed, df$corpus))[1], fold)
  assign(lm_name, this_lm, envir=store_env)
  
  summarise(df,
            log_lik = as.numeric(logLik(this_lm, REML = F)),
            test_lik = logLik_test(this_lm, this_test_data, this_test_data$psychometric),
            test_mse = mse_test(this_lm, this_test_data, this_test_data$psychometric))
}
# For a previously fitted lm stored in store_env, get the residuals on test data of the relevant data subset.
get_lm_residuals <- function(df, fold, store_env) {
  # Retrieve the relevant lm.
  lm_name = paste(unique(paste(df$model, df$training, df$seed, df$corpus))[1], fold)
  this_lm <- get(lm_name, envir=store_env)
  
  mutate(df,
         likelihood = logLik_test_per(this_lm, df, df$psychometric),
         resid = df$psychometric - predict(this_lm, df, re.form=NA))
}
# Compute per-example delta-log-likelihood for the given test fold.
get_lm_delta_log_lik <- function(test_data, fold, baseline_env, full_env) {
  lm_name = paste(unique(paste(test_data$model, test_data$training, test_data$seed, test_data$corpus))[1], fold)
  baseline_lm <- get(lm_name, envir=baseline_env)
  full_lm <- get(lm_name, envir=full_env)
  
  delta_log_lik = logLik_test_per(full_lm, test_data, test_data$psychometric) - logLik_test_per(baseline_lm, test_data, test_data$psychometric)
  return(cbind(test_data, delta_log_lik=delta_log_lik))
}
#####
# Define regression formulae.
# Eye-tracking regression: only use surprisal and previous surprisal; SPRT regression: use 2-back features.

#baseline_rt_regression = psychometric ~ te(freq, len, bs = "cr") + te(prev_freq, prev_len, bs = "cr") + te(prev2_freq, prev2_len, bs = "cr")
#baselie_sprt_regression = psychometric ~ te(freq, len, bs = "cr") + te(prev_freq, prev_len, bs = "cr") + te(prev2_freq, prev2_len, bs = "cr") + te(prev3_freq, prev3_len, bs = "cr") + te(prev4_freq, prev4_len, bs = "cr")

#full_rt_regression = psychometric ~ s(surprisal, bs = "cr", k = 20) + s(prev_surp, bs = "cr", k = 20) + s(prev2_surp, bs = "cr", k = 20) + te(freq, len, bs = "cr") + te(prev_freq, prev_len, bs = "cr") + te(prev2_freq, prev2_len, bs = "cr")
#full_sprt_regression = psychometric ~ s(surprisal, bs = "cr", k = 20) + s(prev_surp, bs = "cr", k = 20) + s(prev2_surp, bs = "cr", k = 20) + s(prev3_surp, bs = "cr", k = 20) + s(prev4_surp, bs = "cr", k = 20) + te(freq, len, bs = "cr") + te(prev_freq, prev_len, bs = "cr") + te(prev2_freq, prev2_len, bs = "cr") + te(prev3_freq, prev3_len, bs = "cr") + te(prev4_freq, prev4_len, bs = "cr")

baseline_rt_regression = psychometric ~ freq + prev_freq + prev2_freq + len + prev_len + prev2_len
baseline_sprt_regression = psychometric ~ freq + prev_freq + prev2_freq + prev3_freq + prev4_freq + len + prev_len + prev2_len + prev3_len + prev4_len

full_sprt_regression = psychometric ~ surprisal + prev_surp + prev2_surp + prev3_surp + prev4_surp + freq + prev_freq + prev2_freq + prev3_freq + prev4_freq + len + prev_len + prev2_len + prev3_len + prev4_len
full_rt_regression = psychometric ~ surprisal + prev_surp + prev2_surp + freq + prev_freq + prev2_freq + len + prev_len + prev2_len
  
#####
# Prepare frames/environments for storing results/objects.
baseline_results = data.frame()
full_model_results = data.frame()
baseline_residuals = data.frame()
full_residuals = data.frame()
log_lik_deltas = data.frame()

#Randomly shuffle the data
all_data<-all_data[sample(nrow(all_data)),]
#Create K equally size folds
K = 10
folds <- cut(seq(1,nrow(all_data)),breaks=K,labels=FALSE)
#Perform 10 fold cross validation

# Fit models for some fold of the data.
baseline_corpus = function(corpus, df, test_data, fold, env) {
  if(corpus == "dundee") {
    get_lm_data(df, test_data, baseline_rt_regression, fold, env)
  } else {
    get_lm_data(df, test_data, baseline_sprt_regression, fold, env)
  }
}
full_model_corpus = function(corpus, df, test_data, fold, env) {
  if(corpus[1] == "dundee") {
    get_lm_data(df, test_data, full_rt_regression, fold, env)
  } else {
    get_lm_data(df, test_data, full_sprt_regression, fold, env)
  }
}

# Prepare a new Environment in which we store fitted LMs, which we'll query later for residuals and other metrics.
baseline_env = new.env()
full_env = new.env()

for(i in 1:K) { 
  #Segement your data by fold using the which() function 
  testIndexes <- which(folds==i, arr.ind=TRUE)
  test_data <- all_data[testIndexes, ]
  train_data <- all_data[-testIndexes, ]
  
  # Compute a baseline linear model for each model--training--seed--RT-corpus combination.
  baselines = train_data %>%
    group_by(model, training, seed, corpus) %>%
      print(model) %>%
      do(baseline_corpus(unique(.$corpus), ., test_data, i, baseline_env)) %>%
    ungroup() %>%
    mutate(seed = as.factor(seed),
           fold = i)
  
  baseline_results = rbind(baseline_results, baselines)
  
  # Compute a full linear model for each model--training--seed-RT-corpus combination
  full_models = train_data %>%
    group_by(model, training, seed, corpus) %>%
      do(full_model_corpus(unique(.$corpus), ., test_data, i, full_env)) %>%
    ungroup() %>%
    mutate(seed = as.factor(seed),
           fold = i)
  
  full_model_results = rbind(full_model_results, full_models)
  
  # Compute delta-log-likelihoods
  fold_log_lik_deltas = test_data %>%
    group_by(model, training, seed, corpus) %>%
      do(get_lm_delta_log_lik(., i, baseline_env, full_env)) %>%
    ungroup()

  log_lik_deltas = rbind(log_lik_deltas, fold_log_lik_deltas)
  
  fold_baseline_residuals = test_data %>%
    group_by(model, training, seed, corpus) %>%
      do(get_lm_residuals(., i, baseline_env)) %>%
    ungroup()

  baseline_residuals = rbind(baseline_residuals, fold_baseline_residuals)

  fold_full_residuals = test_data %>%
    group_by(model, training, seed, corpus) %>%
      do(get_lm_residuals(., i, full_env)) %>%
    ungroup()

  full_residuals = rbind(full_residuals, fold_full_residuals)
}
```

```{r}
#write.csv(full_residuals, "../data/analysis_checkpoints/full_residuals.csv")
#write.csv(baseline_residuals, "../data/analysis_checkpoints/baseline_residuals.csv")
```

```{r}
model_deltas = log_lik_deltas %>%
  group_by(model, training, seed, corpus) %>% 
  summarise(mean_delta_log_lik = mean(delta_log_lik),
            sem_delta_log_lik = sd(delta_log_lik) / sqrt(length(delta_log_lik)))
```

```{r}
write.csv(full_model_results, "../data/analysis_checkpoints/full_model_result.csv")
write.csv(baseline_results, "../data/analysis_checkpoints/baseline_results.csv")
#full_model_results = read.csv("../data/analysis_checkpoints/ffull_model_results.csv")
#baseline_results = read.csv("../data/analysis_checkpoints/fbaseline_resultsb.csv")
```

```{r}
metric <- "ΔLogLik"
#metric <- "-ΔMSE"

# # Select the relevant metric.
model_deltas = model_deltas %>%
    # Retrieve the current test metric
    mutate(delta_test_mean = mean_delta_log_lik,
           delta_test_sem = sem_delta_log_lik) %>%
    # mutate(delta_test_mean = mean_delta_mse,
    #        delta_test_sem = sem_delta_mse)
    
    # Remove the raw metrics.
    select(-mean_delta_log_lik, -sem_delta_log_lik,
           #-mean_delta_mse, -sem_delta_mse
           )
model_deltas
```

```{r}
# Sanity check: training on train+test data should yield improved performance over training on just training data. (When evaluating on test data.)
# full_baselines = all_data %>%
#   group_by(model, training, seed, corpus) %>%
#   summarise(baseline_train_all_test_lik = logLik_test(lm(psychometric ~ len + freq + sent_pos, data=.), semi_join(test_data, ., by=c("training", "model", "seed", "corpus")), semi_join(test_data, ., by=c("training", "model", "seed", "corpus"))$psychometric)) %>%
#   ungroup()
# full_baselines
# 
# full_baselines %>%
#   right_join(baselines, by=c("seed", "training", "model", "corpus")) %>%
#   mutate(delta=baseline_train_all_test_lik-baseline_test_lik) %>%
#   select(-baseline_lik) # %>%
#   #select(-baseline_test_lik, -baseline_train_all_test_lik, -baseline_lik, -baseline_test_mse)
```

# Load language model data (SyntaxGym, PPL)

```{r}
language_model_data = read.csv("../data/model_metadata.csv") %>%
  mutate(model = as.character(model),
         model = if_else(model == "gpt-2", "gpt2", model),
         model = as.factor(model)) %>%
  mutate(train_size = case_when(str_starts(training, "bllip-lg") ~ 42,
                                str_starts(training, "bllip-md") ~ 15,
                                str_starts(training, "bllip-sm") ~ 5,
                                str_starts(training, "bllip-xs") ~ 1),
         
         # Training vocabulary usually covaries with the training corpus.
         # But BPE models share a vocabulary across training corpora.
         training_vocab=as.factor(ifelse(str_detect(training, "gptbpe"), "gptbpe", as.character(training))),
         training_source=as.factor(str_replace(as.character(training), "-gptbpe", ""))
         ) %>%
  mutate(seed = as.factor(seed)) %>%
  select(-pid, -test_loss) %>%
  distinct(model, training, seed, .keep_all = TRUE)
table(language_model_data$seed)
table(model_deltas$seed)
```

First join delta-metric data with model auxiliary data.

```{r}
model_deltas = model_deltas %>%
  merge(language_model_data, by = c("seed", "training", "model"), all=T) %>%
  drop_na()

model_deltas
```

Also join on the original linear model data, rather than collapsing to delta-metrics.
This will support regressions later on that don't collapse across folds.


# Final data preprocessing

```{r Filter models and/or corpora}
# Exclude ordered-neurons from all analyses.
model_deltas <- model_deltas %>%
  filter(model != "ordered-neurons")
```


# Visualizations

## The basics

```{r, fig.cap="Corpus sizes"}
all_data %>% ggplot(aes(x=corpus)) + geom_bar()
print(all_data %>% group_by(corpus) %>% summarise(n=n()))
```


```{r, fig.cap="Word frequency distribution by corpus"}
all_data %>% 
  ggplot(aes(x=freq, color=corpus)) + geom_density()
```

```{r, fig.cap="Word length distribution by corpus"}
all_data %>% 
  ggplot(aes(x=len, color=corpus)) + geom_density()
```

```{r, fig.cap="Surprisal distribution by corpus"}
all_data %>% 
  ggplot(aes(x=surprisal, color=corpus)) + geom_density()
```

## Predictive power and SG


```{r By model}
model_deltas %>%
  ggplot(aes(x=sg_score, y=delta_test_mean)) +
    geom_errorbar(aes(ymin=delta_test_mean-delta_test_sem, ymax=delta_test_mean+delta_test_sem)) +
    geom_smooth(method="lm", se=T) +
    geom_point(stat="identity", position="dodge", alpha=1, size=3, aes(color=training_vocab, shape=model)) +
    ylab(metric) +
    xlab("Syntax Generalization Score") +
    ggtitle("Syntactic Generalization vs. Predictive Power") +
    scale_color_manual(values = c("bllip-lg"="#440154FF",
                              "bllip-md"="#39568CFF",
                              "bllip-sm"="#1F968BFF",
                              "bllip-xs"="#73D055FF",
                              "gptbpe"="#888888")) +
    facet_grid(~corpus, scales="free") +
    theme(axis.text=element_text(size=14),
          strip.text.x = element_text(size=14),
          legend.text=element_text(size=14),
          axis.title=element_text(size=18),
          legend.position = "bottom")
#ggsave("./cogsci_images/sg_loglik.png",height=5,width=6)
```

### Regression analyses

We control for effects of perplexity by relating the residuals of a `performance ~ PPL` regression to SG score.

```{r Residualized regression}
# Prepare a residualized regression for x1 onto y, controlling for the effects of x2.
d_resid = model_deltas %>%
  drop_na() %>%
  
  group_by(corpus) %>%
    # Residualize delta metric w.r.t PPL for each model--training--seed within
    # training vocabulary
    mutate(resid.delta = resid(lm(delta_test_mean ~ training_vocab:test_ppl))) %>%
    # Residualize SG score w.r.t. PPL for each model--training--seed
    # within training vocabulary
    mutate(resid.sg = resid(lm(sg_score ~ training_vocab:test_ppl))) %>%
  ungroup()


# Now plot residual vs SG
d_resid %>%
  #filter(corpus != "bnc-brown") %>%
  ggplot(aes(x=resid.sg, y=resid.delta)) +
    theme_bw() +
    scale_shape_manual(values = c(21, 24, 22, 23)) +
    geom_smooth(method="lm", se=T, alpha=0.3) +
    geom_point(stat="identity", position="dodge", alpha=1, size=5, aes(shape = model, fill=training_source, color = training_vocab, stroke = 1)) +
    ylab(paste("Residual", metric)) +
    xlab("Residual Syntax Generalization Score") +
    ggtitle("Syntactic Generalization vs. Predictive Power") +
    labs(color="training") + 
    scale_color_manual(values = c("bllip-lg"="#440154FF",
                                  "bllip-md"="#39568CFF",
                                  "bllip-sm"="#1F968BFF",
                                  "bllip-xs"="#73D055FF",
                                  "gptbpe"="#f0941f")) +
  scale_fill_manual(values = c("bllip-lg"="#440154FF",
                                  "bllip-md"="#39568CFF",
                                  "bllip-sm"="#1F968BFF",
                                  "bllip-xs"="#73D055FF",
                                  "gptbpe"="#f0941f"), guide=F) +
    facet_grid(.~corpus, scales="free") +
    theme(axis.text=element_text(size=14),
          strip.text.x = element_text(size=14),
          legend.text=element_text(size=14),
          axis.title=element_text(size=18),
          legend.position = "right")
ggsave("../images/cuny2020/dll_sg.pdf",height=4.5,width=9, device = cairo_pdf)
```


```{r Stepwise regression}
do_stepwise_regression = function(cur_corpus) {
  regression_data = model_deltas %>%
    filter(corpus == cur_corpus)
  
  print("----------------------")
  print(cur_corpus)
  
  lm1 = lm(delta_test_mean ~ training_vocab:test_ppl, data = regression_data)
  lm2 = lm(delta_test_mean ~ training_vocab:test_ppl + sg_score, data = regression_data)
  print(anova(lm1, lm2))
  summary(lm2)
}
do_stepwise_regression("bnc-brown")
do_stepwise_regression("dundee")
do_stepwise_regression("natural-stories")
```

```{r Sanity check: equivalence between analyses}
# The residualized analysis and the stepwise regression analysis
# should yield the same coefficients for the SG score variable.
#
# Below, we compute the slope coefficient for the SG term in the
# residualized analyses.
#
# These coefficients should match those found in the stepwise
# regression for `sg_score` above.
d_resid %>% group_by(corpus) %>%
  group_modify(~tidy(lm(resid.delta ~ training_vocab:test_ppl + resid.sg, data=.))
                 %>% filter(term == "resid.sg")) %>% 
  select(corpus, estimate)
```

## Predictive power and perplexity

```{r}

model_deltas %>%
  mutate(test_ppl = if_else(test_ppl > 500, 329.9, test_ppl),
         bpe = if_else(training_vocab == "gptbpe", "yes", "no")) %>%
  ggplot(aes(x=test_ppl, y=delta_test_mean, shape = model, ymin=0)) +
    theme_bw() +
    geom_text(aes(x=275, y=0, label = c("//"))) +
    geom_errorbar(aes(ymin=delta_test_mean-delta_test_sem, ymax=delta_test_mean+delta_test_sem, color=training_vocab), alpha=0.4) +
    #geom_smooth(method="lm", se=F) +
    geom_point(stat="identity", position="dodge", alpha=1, size=5, aes(fill=training_source, color = training_vocab, stroke = 1)) +
    ylab("ΔLogLik per token") +
    xlab("Test Perplexity") +
    #coord_cartesian(ylim = c(1, 16)) +
    ggtitle("Test Perplexity vs. Predictive Power") +
    labs(color="training") + 
    scale_color_manual(values = c("bllip-lg"="#440154FF",
                                  "bllip-md"="#39568CFF",
                                  "bllip-sm"="#1F968BFF",
                                  "bllip-xs"="#73D055FF",
                                  "gptbpe"="#f0941f")) +
  scale_fill_manual(values = c("bllip-lg"="#440154FF",
                                  "bllip-md"="#39568CFF",
                                  "bllip-sm"="#1F968BFF",
                                  "bllip-xs"="#73D055FF",
                                  "gptbpe"="#f0941f"), guide=F) +
    scale_shape_manual(values = c(21, 24, 22, 23)) +
    scale_x_continuous(labels=c(0, 50, 100, 150, 200, 250, 500 ,550), breaks=c(0, 50, 100, 150, 200, 250, 300, 350), minor_breaks = NULL) +
    scale_y_continuous(limits = c(0, NA), expand = c(0,0)) +
    facet_wrap(~corpus, scales="free") +
    coord_cartesian(clip="off") +
    theme(axis.text=element_text(size=12),
          strip.text.x = element_text(size=12),
          legend.text=element_text(size=12),
          axis.title=element_text(size=12),
          legend.position = "right")
ggsave("../images/cuny2020/ppl_loglik.pdf",height=5,width=12, device = cairo_pdf)

```

```{r, eval=False}

dll_cor_test = function(df){
  df %>%
    summarise(
      cor = cor.test(df$delta_test_mean, df$test_ppl)$estimate,
      p = cor.test(df$delta_test_mean, df$test_ppl)$p.value
    )
}

model_deltas %>%
  filter(model != "5gram") %>%
  group_by(training, corpus) %>%
    mutate(n = n()) %>%
  ungroup() %>%
  filter(n > 2) %>%
  group_by(training, corpus) %>%
    do({ dll_cor_test(.) }) %>%
  ungroup() %>%
  arrange(corpus)

```

```{r}
lmd = model_deltas %>%
  #filter(model != "5gram") %>%
  mutate(training_vocab=ifelse(str_detect(as.character(training), "gptbpe"),
                               "gptbpe", as.character(training)))
summary(lmer(delta_test_mean ~ training_vocab:test_ppl + (1 | corpus) + (1 | model), data=lmd))
```


## Effect of training data size

```{r On predictive power, eval=False}
model_deltas %>%
  mutate(train_size = log(train_size)) %>%
  mutate(bpe = if_else(training_vocab == "gptbpe", "yes", "no"),
         bpe = as.factor(bpe)) %>%
  ggplot(aes(x=train_size, y=delta_test_mean, color=model)) +
    theme_bw() +
    geom_errorbar(aes(ymin=delta_test_mean-delta_test_sem, ymax=delta_test_mean+delta_test_sem), width = 0.1) +
    geom_smooth(method="lm", se=T, alpha=0.2) +
    geom_point(stat="identity", position="dodge", alpha=1, size=3, aes(shape=bpe)) +
    ylab("ΔLogLik per token) +
    xlab("Log Million Training Tokens") +
    ggtitle("Training Size vs. Predictive Power") +
    facet_grid(.~corpus, scales="free") +
    #scale_color_manual(values = c("#A42EF1", "#3894C8")) +
    theme(axis.text=element_text(size=12),
          strip.text.x = element_text(size=12),
          legend.text=element_text(size=8),
          legend.title=element_text(size=8),
          axis.title=element_text(size=14),
          legend.position = "bottom",
          legend.direction = "horizontal",
          legend.key.width = unit(0.3,"cm"),
          legend.spacing.x = unit(0.1, 'cm'))
#ggsave("../images/cuny2020/training_loglik.png",height=5,width=5)
```

```{r, eval=False}

model_cor_test = function(df){
  df %>%
    summarise(cor = cor.test(df$train_size, df$delta_test_mean)$estimate,
              p = cor.test(df$train_size, df$delta_test_mean)$p.value)
}

model_deltas %>%
  group_by(model, corpus) %>%
    do({model_cor_test(.)}) %>%
  ungroup() %>%
  arrange()


```

```{r PPL vs. SG score}

model_deltas %>%
  mutate(test_ppl = if_else(test_ppl > 500, 329.9, test_ppl)) %>%
  mutate(train_size = log(train_size)) %>%
  mutate(bpe = if_else(training_vocab == "gptbpe", "yes", "no"),
         bpe = as.factor(bpe)) %>%
  ggplot(aes(x=test_ppl, y=sg_score)) +
    theme_bw() +
    geom_hline(yintercept = 0.28, linetype = "dashed", color="gray") +
    geom_text(aes(x=240, y=0.3), label="random", color="gray") +
    geom_point(stat="identity", position="dodge", alpha=0.3, size=4, aes(shape = model, fill=training_source, color = training_vocab, stroke = 1)) +
    geom_text(aes(x=275, y=0, label = c("//"))) +
    ylab("SG Score") +
    xlab("Test Perplexity") +
    ggtitle("Test PPL vs. SG Score") +
    labs(color="training") + 
    scale_color_manual(values = c("bllip-lg"="#440154FF",
                                  "bllip-md"="#39568CFF",
                                  "bllip-sm"="#1F968BFF",
                                  "bllip-xs"="#73D055FF",
                                  "gptbpe"="#f0941f")) +
    scale_fill_manual(values = c("bllip-lg"="#440154FF",
                                  "bllip-md"="#39568CFF",
                                  "bllip-sm"="#1F968BFF",
                                  "bllip-xs"="#73D055FF",
                                  "gptbpe"="#f0941f"), guide=F) +
    scale_shape_manual(values = c("5gram"=21, vanilla=22, gpt2=24, rnng=23)) +
    scale_x_continuous(labels=c(0, 50, 100, 150, 200, 250, 500 ,550), breaks=c(0, 50, 100, 150, 200, 250, 300, 350), minor_breaks = NULL) +
    scale_y_continuous(limits = c(0, 1), expand = c(0,0)) +
    theme(axis.text=element_text(size=12),
          strip.text.x = element_text(size=12),
          legend.text=element_text(size=8),
          legend.title=element_text(size=8),
          axis.title=element_text(size=14),
          legend.position = "none",
          legend.direction = "horizontal",
          legend.key.width = unit(0.3,"cm"),
          legend.spacing.x = unit(0.1, 'cm'))
ggsave("../images/cuny2020/ppl_sg.pdf",height=4.5,width=3, device = cairo_pdf)
```


```{r Size vs. SG score, eval=False}
model_deltas %>%
  mutate(train_size = log(train_size)) %>%
  mutate(bpe = if_else(training_vocab == "gptbpe", "yes", "no"),
         bpe = as.factor(bpe)) %>%
  ggplot(aes(x=train_size, y=sg_score, color=model)) +
    theme_bw() +
    geom_smooth(method="lm", se=T, alpha=0.2) +
    geom_point(stat="identity", position="dodge", alpha=1, size=3, aes(shape=bpe)) +
    ylab("SG SCore") +
    xlab("Log Million Training Tokens") +
    ggtitle("Training Size vs. SG Score") +
    #scale_color_manual(values = c("#A42EF1", "#3894C8")) +
    #facet_grid(~model, scales="free") +
    theme(axis.text=element_text(size=12),
          strip.text.x = element_text(size=12),
          legend.text=element_text(size=8),
          legend.title=element_text(size=8),
          axis.title=element_text(size=14),
          legend.position = "bottom",
          legend.direction = "horizontal",
          legend.key.width = unit(0.3,"cm"),
          legend.spacing.x = unit(0.1, 'cm'))
#ggsave("../images/cuny2020/training_sg.png",height=5,width=4)
```


```{r, eval=False}

model_cor_test = function(df){
  df %>%
    summarise(cor = cor.test(df$train_size, df$sg_score)$estimate,
              p = cor.test(df$train_size, df$sg_score)$p.value)
}

model_deltas %>%
  group_by(model) %>%
    do({model_cor_test(.)}) %>%
  ungroup()



```

## Smith & Levy reproduction

#### This redone so that it's unique for each model
```{r, eval=False}
all_data %>%
  ggplot(aes(x=surprisal, color=model)) +
  theme_bw() +
  geom_density() +
  facet_wrap(.~corpus, ncol=1, scales="free", strip.position = "right") +
  coord_cartesian(xlim = c(0, 25)) +
  ggtitle("Distribution of Surprisal") +
ggsave("../images/cuny2020/surp_corr_marginals.png",height=5,width=4)

```

```{r Fit GAMs, eval=False}
k = 1.97

# Fit a GAM for a bootstrap sample.
fit_gam_inner = function(bootstrap_sample, key) {
  # This bootstrap sample may have repeated elements. That causes a problem for
  # mgcv, which internally cross-validates some model parameters -- it may
  # allocate repeated elements to different folds and thus double-dip. We'll
  # prevent this by instead providing the whole (pre-bootstrap) dataset to mgcv,
  # and using `weights` to constrain which elements are seen, and how many
  # times. (Repeated elements of the sample may get a weight of 2 or 3 or N,
  # which is exactly what we want.)
  
  # rsplit$data contains the original entire dataset.
  df = bootstrap_sample$data
  # as.integer.rsplit returns the indices of the examples which are in-sample.
  # convert this to a count vector, with dimension N (total dataset rows)
  weights = tabulate(as.integer(bootstrap_sample), nrow(df))
  
  if (key$corpus == "dundee") {
    # Reading time regression: use features of current and previous word
    m = gam(psychometric ~ s(surprisal, bs = 'cr', k = 20) + s(prev_surp, bs = 'cr', k = 20) +
                           te(freq, len, bs = 'cr') + te(prev_freq, prev_len, bs = 'cr'),
            data = df, weights = weights)
    
    terms_to_predict = c("s(surprisal)", "s(prev_surp)")
  } else {
    # SPRT regression: use features of current and 3 previous words
    m = gam(psychometric ~ s(surprisal, bs = 'cr', k = 20) + s(prev_surp, bs = 'cr', k = 20) +
                           s(prev2_surp, bs = 'cr', k = 20) + s(prev3_surp, bs = 'cr', k = 20) +
                           te(freq, len, bs = 'cr') + te(prev_freq, prev_len, bs = 'cr') +
                           te(prev2_freq, prev2_len, bs = 'cr') + te(prev3_freq, prev3_len, bs = 'cr'),
            data = df, weights = weights)
    
    terms_to_predict = c("s(surprisal)", "s(prev_surp)",
                         "s(prev2_surp)", "s(prev3_surp)")
  }

  # Produce psychometric predictions line using just the relevant context-specific predictors.

  newdata = data.frame(surprisal=seq(0,20,by=0.1),
                       prev_surp=seq(0,20,by=0.1),
                       prev2_surp=seq(0,20,by=0.1),
                       prev3_surp=seq(0,20,by=0.1),
                       freq=0, prev_freq=0, prev2_freq=0, prev3_freq=0,
                       len=0, prev_len=0, prev2_len=0, prev3_len=0)
  
  # Returns a matrix N_samples * N_terms.
  per_term_predictions = predict(m, newdata=newdata, terms=terms_to_predict, type="terms")
  
  # Additive model -- sum across predictor response contributions (matrix columns).
  predictions = rowSums(per_term_predictions)
  
  return(newdata %>% mutate(y=predictions))
}

# Fit a bootstrap-re-estimated GAM for the given model--corpus--training group.
fit_gam = function(df, key, alpha=0.05) {
  # Bootstrap-resample data
  boot_models = df %>% bootstraps(times=50) %>% 
    # Fit a GAM and get predictions for each sample
    mutate(smoothed=map(splits, fit_gam_inner, key=key))
  
  # Extract mean and 5% and 95% percentile y-values for each surprisal value
  result = boot_models %>% 
    unnest(smoothed) %>% 
    select(surprisal, y) %>% 
    group_by(surprisal) %>% 
      summarise(y_lower=quantile(y, alpha / 2), 
                y_upper=quantile(y, 1 - alpha / 2),
                y=mean(y)) %>% 
    ungroup()
  
  return (result)
}

smooths = all_data %>%
  mutate(
    training_vocab=as.factor(ifelse(str_detect(training, "gptbpe"), "gptbpe", as.character(training))),
    training_source=as.factor(str_replace(as.character(training), "-gptbpe", ""))) %>%
  group_by(training_vocab, training_source, model, corpus) %>%
    group_modify(fit_gam) %>%
  ungroup()
write.csv(smooths, "../data/gam_smooths.csv")


```

```{r, eval=False}

smooths %>%
  mutate(corp_model = paste(corpus, model, sep=" / "),
         corp_model = as.factor(corp_model)) %>%
  #filter(training_vocab == "bllip-xs" | training_vocab == "gptbpe" | training_vocab == "bllip-lg") %>%
  #filter(training != "bllip-md-gptbpe" & training != "bllip-sm-gptbpe") %>%
  mutate(bpe = if_else(training_vocab == "gptbpe", "yes", "no"),
         # training = as.character(training),
         # training = if_else(training == "bllip-lg-gptbpe", "bllip-lg", training),
         # training = if_else(training == "bllip-md-gptbpe", "bllip-md", training),
         # training = if_else(training == "bllip-sm-gptbpe", "bllip-sm", training),
         # training = if_else(training == "bllip-xs-gptbpe", "bllip-xs", training)
         ) %>%
  ggplot(aes(x=surprisal, y=y)) +
      theme_bw() +
      geom_line(size=0.5, aes(color=training_vocab, linetype=bpe)) +
      #geom_line(aes(y=y_lower), linetype="dashed") +
      geom_ribbon(aes(ymin=y_lower,ymax=y_upper), alpha=0.2) +
      #geom_line(aes(y=y_upper), linetype="dashed") +
      facet_wrap(~corp_model, ncol=4, scales="free") +
      scale_color_manual(values = c("bllip-lg"="#440154FF",
                                    "bllip-md"="#39568CFF",
                                    "bllip-sm"="#1F968BFF",
                                    "bllip-xs"="#73D055FF",
                                    "gptbpe"="#f0941f")) +
      scale_x_continuous(labels=c(0, 10, 20), breaks=c(0, 10, 20), minor_breaks = NULL) +
      #scale_y_continuous(limits=c(200, 350)) +
      ylab("Slow-down due to surprisal (ms)") +
      ggtitle("Surprisal slow-down effects") +
      theme(legend.position = "left",
            strip.background = element_rect(fill="white"),
            strip.placement = "outside") +
      labs(x="Surprisal", color="Training\nvocabulary", linetype="BPE")
ggsave("../images/cuny2020/gam_surp_corr.png", height=5,width=8)

```



```{r}
ymin = -40
ymax = 100
xmin = 0
xmax = 20

get_d_points = function(df, model, training, corpus){
  x = density(df$surprisal)$x
  y = density(df$surprisal)$y
  return(data.frame(model, training, corpus, x, y))
}

density_data = all_data %>%
  mutate(model = recode(model, vanilla="lstm")) %>%
  group_by(model, training, corpus) %>%
    do({get_d_points(., unique(.$model), unique(.$training), unique(.$corpus))}) %>%
  ungroup() %>%
  mutate(training_vocab=as.factor(ifelse(str_detect(training, "gptbpe"), "gptbpe", as.character(training))),
         training_source=as.factor(str_replace(as.character(training), "-gptbpe", "")),
         bpe=training_vocab == "gptbpe",) %>% 
  filter(x>0, x<20)

smooths = read.csv("../data/gam_smooths.sl2013.all.csv")
gam_smooths = smooths %>% 
  # Plot niceties
  # vanilla -> lstm
  mutate(model=recode(model, vanilla="lstm")) %>% 
  # Create BPE vs non-BPE variable
  mutate(bpe=training_vocab == "gptbpe") %>% 
  
  # Fix 0 surprisal = 0 ms
  group_by(training_vocab, model, corpus) %>% 
    mutate(delta=0 - y[1],
           # Trim lower bound to make sure it gets plotted within the ylim
           y_lower=pmax(ymin, y_lower + delta),
           y=y + delta,
           # Trim upper bound likewise
           y_upper=pmin(ymax, y_upper + delta)) %>% 
  ungroup()

density_data %>% filter(model == "gpt2", training_source == "bllip-lg")

ggplot() +
  theme_bw() +
    annotate("rect", xmin=0, xmax=20, ymin=-40,ymax=-15, fill="grey", alpha=0.3) +
    geom_line(data = density_data, aes(x=x, y=y*200 - 40, linetype=bpe), color="grey") + #, size = 0.1) +# 0.5) +
    geom_line(data = gam_smooths, aes(x=surprisal, y=y, linetype=bpe, color=training_source), size=0.5) +
    geom_ribbon(data = gam_smooths, aes(x=surprisal, ymin=y_lower, ymax=y_upper, fill=training_source, color=NA, linetype=bpe), alpha=0.3) +
    facet_grid(corpus ~ training_source + model, scales="free") +
    ylim(ymin, ymax) +
    coord_cartesian(xlim=c(0,20)) +
    scale_color_manual(values = c("bllip-lg"="#440154FF", "bllip-md"="#39568CFF", "bllip-sm"="#1F968BFF", "bllip-xs"="#73D055FF", "gptbpe"="#f0941f")) +
    scale_fill_manual(values = c("bllip-lg"="#440154FF", "bllip-md"="#39568CFF",  "bllip-sm"="#1F968BFF",  "bllip-xs"="#73D055FF", "gptbpe"="#f0941f"), guide="none") +
    scale_x_continuous(labels=c(0, 10, 20), breaks=c(0, 10, 20), minor_breaks = NULL) +
    theme(legend.position = "bottom") +
    labs(x="Surprisal (bits)", y="Slowdown due to surprisal (ms)",
         linetype="BPE",
         color="Training data")
 ggsave("../images/cuny2020/gam_surp_corr_full.pdf", height=5.5,width=12, device = cairo_pdf)


```







